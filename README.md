# Triplet Entropy Loss

## Abstract
Spoken language identification systems form an integral part in many speech recognition tools today. Over the years many techniques have been used to identify the language spoken given just the audio input, but in recent years the trend has been to use end to end deep learning systems. Most of these techniques involve converting the audio signal into a spectrogram which can be fed into a Convolutional Neural Network which can then predict the spoken language. This technique performs very well when the data being fed to model originates from the same domain as the training examples, but as soon as the input comes from a different domain these systems tend to perform poorly. Examples could be when these systems were trained on WhatsApp recordings but are put into production in an environment where the system receives recordings from a phone line. 

The research presented investigates several methods to improve the generalization of language identification systems to new speakers and to new domains. These methods involve Spectral augmentation, where spectrograms are masked in the frequency or time bands training and by using CNN architectures that are pre-trained on the Imagenet dataset. The primary method investigated though was the Triplet Entropy Loss training method, which involves training a network simultaneously using Cross Entropy and Triplet loss. Several tests were run with three different CNN architecture to investigate what the effect of all three methods have on the generalization of a LID system.

The tests were done in a South African context on six languages, namely Afrikaans, English, Sepedi, Setswanna, Xhosa and Zulu. The two domains tested was data from the NCHLT speech corpus, used as the training domain, with the Lwazi speech corpus being the test domain. 

It was found that all three of these methods improved the generalization of the models tested, though not significantly. Even though the models trained using Triplet Entropy Loss showed a better understanding of the languages, it appears the models still memorize words rather than learning the finer nuances of a language. The research mainly showed that Triplet Entropy Loss has great potential and should be investigated further, but not only in language identification tasks but self supervised tasks as well.

## TEL overview

For tasks such as language identification where the input data can contain data which is present in many other classes as well, such as someone speaking a mix of words from different languages, it will be more optimal to have a loss function that interprets interactions between classes at the output. The loss must optimize the network by learning these interactions between classes to generalize better to the instances where there is a tiny threshold between the classes. A loss that loosely fits this description is the Triplet loss function. By using the Triplet loss function, the weights of a network are being optimized by comparing different classe embeddings with one another and optimizing the distance between the embeddings such that different classes are far from one another. The model can then learn special characteristics of all the classes and in doing so could be able to better learn the fine connections between languages such as English and Zulu for instance. But Triplet loss does not optimize for prediction capabilities directly.

The TEL training method aims to leverage both the strengths of Cross Entropy Loss (CEL) and Triplet loss during the training process, assuming that it would lead to better generalization for language identification tasks. The TEL method though does not contain a pre-training step, but trains simultaneously with both CEL and Triplet losses, as shown in the figure below. As seen, the final embedding layer feeds into two separate layers where each of these output layers are connected to two different losses. TEL can be represented by \Cref{tel_formula}, with $\sigma$ being the softmax function and $g()$ the final classification layer. $N$ is the number of examples in the batch being passed through the network. The embeddings generated for the anchor, postive and negative triplets are given by $f_{i}^{a}$, $f_{i}^{p}$ and $f_{i}^{n}$. If only CEL is used, $f_{i}^{a}$ will be the vector analysed. 



